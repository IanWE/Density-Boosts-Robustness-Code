{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13c7aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from core import utils\n",
    "from core import model_utils, data_utils, constants\n",
    "from sklearn.model_selection import train_test_split\n",
    "from core.attack_utils import mimicry\n",
    "from core import attack_utils\n",
    "from datetime import datetime\n",
    "\n",
    "from core.attack_utils import mimicry\n",
    "\n",
    "#Calculate AUT base on f1 list\n",
    "def aut_score(f1_list):\n",
    "    aut = 0\n",
    "    for i in range(len(f1_list)-1):\n",
    "        aut += (f1_list[i]+f1_list[i+1])/2\n",
    "    aut = aut/(len(f1_list)-1)\n",
    "    print(aut)\n",
    "    return aut\n",
    "\n",
    "#Evaluate AUT\n",
    "def evaluate_aut(model,x_test,y_test,year='2015'):\n",
    "    samples_info = pd.read_json(os.path.join(constants.DREBIN_DATA_DIR,\"extended-features-meta.json\"))\n",
    "    timelines = np.array([\n",
    "                 '2015-01','2015-02','2015-03','2015-04','2015-05','2015-06',\n",
    "                 '2015-07','2015-08','2015-09','2015-10','2015-11','2015-12',\n",
    "                 '2016-01','2016-02','2016-03','2016-04','2016-05','2016-06',\n",
    "                 '2016-07','2016-08','2016-09','2016-10','2016-11','2016-12',\n",
    "                 '2017-01','2017-02','2017-03','2017-04','2017-05','2017-06',\n",
    "                 '2017-07','2017-08','2017-09','2017-10','2017-11','2017-12',\n",
    "                 '2018-01','2018-02','2018-03','2018-04','2018-05','2018-06',\n",
    "                 '2018-07','2018-08','2018-09','2018-10','2018-11','2018-12',\n",
    "                 '2019-01'])\n",
    "    test_samples = samples_info[(samples_info.dex_date>year)]\n",
    "    timelines = timelines[timelines>year]\n",
    "    f1_list = []\n",
    "    goodware_count = []\n",
    "    malware_count = []\n",
    "    for i in range(len(timelines)-1):\n",
    "        indicies = (test_samples.dex_date>timelines[i])&(test_samples.dex_date<timelines[i+1])\n",
    "        x_t,y_t = x_test[indicies],y_test[indicies]\n",
    "        if isinstance(model,LinearSVC):\n",
    "            r = model.predict(x_t)\n",
    "        else:\n",
    "            r = model.predict(x_t)>0.5\n",
    "        f1_list.append(f1_score(y_t,r))\n",
    "        goodware_count.append(y_t.shape[0]-y_t.sum())\n",
    "        malware_count.append(y_t.sum())\n",
    "    return f1_list, aut_score(f1_list)\n",
    "\n",
    "def evaluate_aut_r(r,y_test,year='2015'):\n",
    "    samples_info = pd.read_json(os.path.join(constants.DREBIN_DATA_DIR,\"extended-features-meta.json\"))\n",
    "    timelines = np.array([\n",
    "                 '2015-01','2015-02','2015-03','2015-04','2015-05','2015-06',\n",
    "                 '2015-07','2015-08','2015-09','2015-10','2015-11','2015-12',\n",
    "                 '2016-01','2016-02','2016-03','2016-04','2016-05','2016-06',\n",
    "                 '2016-07','2016-08','2016-09','2016-10','2016-11','2016-12',\n",
    "                 '2017-01','2017-02','2017-03','2017-04','2017-05','2017-06',\n",
    "                 '2017-07','2017-08','2017-09','2017-10','2017-11','2017-12',\n",
    "                 '2018-01','2018-02','2018-03','2018-04','2018-05','2018-06',\n",
    "                 '2018-07','2018-08','2018-09','2018-10','2018-11','2018-12',\n",
    "                 '2019-01'])\n",
    "    test_samples = samples_info[(samples_info.dex_date>year)]\n",
    "    timelines = timelines[timelines>year]\n",
    "    f1_list = []\n",
    "    goodware_count = []\n",
    "    malware_count = []\n",
    "    for i in range(len(timelines)-1):\n",
    "        indicies = (test_samples.dex_date>timelines[i])&(test_samples.dex_date<timelines[i+1])\n",
    "        r_t,y_t = r[indicies],y_test[indicies]\n",
    "        f1_list.append(f1_score(y_t,r_t))\n",
    "        #print(f1_list[-1])\n",
    "        goodware_count.append(y_t.shape[0]-y_t.sum())\n",
    "        malware_count.append(y_t.sum())\n",
    "        #print(goodware_count[-1],malware_count[-1])\n",
    "    return f1_list, aut_score(f1_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0f590c",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20deaa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_info = pd.read_json(\"../transcend/extended-features-meta.json\")\n",
    "x_train_,y_train_,x_test,y_test = data_utils.load_drebin_dataset('2015',False)\n",
    "features, feature_names, name_feat, feat_name = data_utils.load_features([],'drebin','2015',False)\n",
    "x_train,x_val,y_train,y_val = train_test_split(x_train_,y_train_,test_size=0.05,random_state = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90f8c1f",
   "metadata": {},
   "source": [
    "## Mark which features are modifiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f29f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#note api features\n",
    "api_idx = [i for i in range(len(feature_names)) if 'calls' in feature_names[i].split('::')[0]]\n",
    "removable_fs = [i for i in range(len(feature_names)) if 'api_calls' in feature_names[i].split('::')[0]\n",
    "               or 'activities' in feature_names[i].split('::')[0] \n",
    "                or 's_and_r' in feature_names[i].split('::')[0]\n",
    "               or 'providers' in feature_names[i].split('::')[0]]\n",
    "manipulation = np.zeros(x_train_.shape[1])\n",
    "manipulation[removable_fs] = 1\n",
    "apis = np.zeros(x_train_.shape[1])\n",
    "apis[api_idx] = 1\n",
    "joblib.dump([apis,manipulation],\"materials/feat_info.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee89e47",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52c6a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('models/drebin/linearsvm_drebin_full.pkl'):\n",
    "    basesvm = model_utils.load_model('linearsvm','drebin','models/drebin/','linearsvm_drebin_full')\n",
    "else:\n",
    "    basesvm = model_utils.train_model('linearsvm','drebin',x_train,y_train,x_test,y_test)\n",
    "    model_utils.save_model('linearsvm',basesvm,'models/drebin/','linearsvm_drebin_full')\n",
    "r = basesvm.predict(x_test)\n",
    "print(classification_report(r,y_test,digits=5))\n",
    "f1_list, aut = evaluate_aut(basesvm,x_test,y_test,year='2015')\n",
    "print(aut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2096d0",
   "metadata": {},
   "source": [
    "#### mimicry attacks on SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6239a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = basesvm.predict(x_train)\n",
    "indicies = np.where((r!=0)&(y_train!=0))[0][:1000]\n",
    "ben_x = x_train[np.where((r==0)&(y_train==0))]\n",
    "\n",
    "success_flag,x_mod = mimicry(basesvm, x_train[indicies][:500], ben_x, torch.Tensor(manipulation), None, 1)\n",
    "print((success_flag==1).sum()/len(success_flag))\n",
    "success_flag,x_mod = mimicry(basesvm, x_train[indicies][:500], ben_x, torch.Tensor(manipulation), None, 10)\n",
    "print((success_flag==1).sum()/len(success_flag))\n",
    "success_flag,x_mod = mimicry(basesvm, x_train[indicies][:500], ben_x, torch.Tensor(manipulation), None, 30)\n",
    "print((success_flag==1).sum()/len(success_flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985f34a6",
   "metadata": {},
   "source": [
    "#### Draw the distribution of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d0a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = np.array(x_train_[:,idx].sum(axis=0))[0,:]\n",
    "sorted_weight = abs(basesvm.coef_[0]).argsort()\n",
    "x= x_train_[:,sorted_weight[-1000:]]\n",
    "\n",
    "l = [0,1,2,5,10,50,100,10000000]\n",
    "lst = []\n",
    "for i in range(len(l)-1):\n",
    "    lst.append(x[(x.sum(axis=0)>l[i])&(x.sum(axis=0)<=l[i+1])].shape[1])\n",
    "\n",
    "# 假设你有一些数据\n",
    "y = lst\n",
    "label = ['1','2','5','10','50','100','>100']\n",
    "plt.rcParams['font.sans-serif'] = ['Times New Roman']\n",
    "x = np.arange(len(y))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.style.use('bmh')\n",
    "plt.grid(axis='x')\n",
    "\n",
    "# 设置背景透明度\n",
    "ax.patch.set_alpha(0)\n",
    "\n",
    "plt.xlabel('Occurrences', fontsize=14)\n",
    "plt.ylabel('Feature counts', fontsize=14)\n",
    "ax.bar(x, y, hatch='//')\n",
    "plt.xticks(x, label,size=14,rotation=0)\n",
    "plt.yticks(size=14)\n",
    "#plt.ylim(330000,430000)\n",
    "fig.subplots_adjust(bottom=0.5)\n",
    "# 设置坐标轴的背景透明度和颜色\n",
    "ax.xaxis.set_tick_params(color='black', labelcolor='black', pad=10)\n",
    "#\n",
    "plt.savefig(\"images/drebin_features.png\",bbox_inches='tight')\n",
    "plt.savefig(\"images/drebin_features.pdf\",bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c586b65",
   "metadata": {},
   "source": [
    "## NN-Selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc785a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "selector = SelectFromModel(basesvm, prefit=True, max_features=1000)\n",
    "idx = np.where(selector.get_support()==True)[0]\n",
    "\n",
    "x_train_selected = np.array(x_train[:,idx].todense())\n",
    "x_test_selected = np.array(x_test[:,idx].todense())\n",
    "x_val_selected = np.array(x_val[:,idx].todense())\n",
    "\n",
    "if os.path.exists('models/drebin/nn_drebin_selected.pkl'):\n",
    "    nn_selected = model_utils.load_model('nn','drebin','models/drebin/','nn_drebin_selected',x_train_selected.shape[1])\n",
    "else:\n",
    "    nn_selected = model_utils.train_model('nn','drebin',x_train_selected,y_train,x_val_selected,y_val,300,'')\n",
    "    model_utils.save_model('nn',nn_selected,'models/drebin/','nn_drebin_selected')\n",
    "r=nn_selected.predict(x_test_selected)>0.5\n",
    "print(classification_report(r,y_test,digits=5))\n",
    "f1_list_selected, aut_selected = evaluate_aut(nn_selected,x_test_selected,y_test,year='2015')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4c34ce",
   "metadata": {},
   "source": [
    "#### Saving modifiable features for PAD and backdoor attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0200a8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump([x_train_selected, y_train, x_val_selected, y_val, x_test_selected, y_test],'materials/drebin_selected.pkl')\n",
    "features, feature_names, name_feat, feat_name = data_utils.load_features([],'drebin','2015',False,idx)\n",
    "\n",
    "api_idx = [i for i in range(len(feature_names)) if 'calls' in feature_names[i].split('::')[0]]\n",
    "removable_fs = [i for i in range(len(feature_names)) if 'api_calls' in feature_names[i].split('::')[0]\n",
    "               or 'activities' in feature_names[i].split('::')[0] \n",
    "                or 's_and_r' in feature_names[i].split('::')[0]\n",
    "               or 'providers' in feature_names[i].split('::')[0]]\n",
    "manipulation = np.zeros(1000)\n",
    "manipulation[removable_fs] = 1\n",
    "apis = np.zeros(1000)\n",
    "apis[api_idx] = 1\n",
    "joblib.dump([apis,manipulation],\"materials/feat_info_selected.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2325fb2b",
   "metadata": {},
   "source": [
    "#### Mimicry on NN-selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f1_list_selected,aut_selected)##44451\n",
    "indicies = np.where((r!=0)&(y_test!=0))[0][:1000]\n",
    "r_train = nn_selected.predict(x_train_selected)>0.5\n",
    "ben_x = x_train_selected[np.where((r_train==0)&(y_train==0))]\n",
    "\n",
    "success_flag,x_mod = mimicry(nn_selected, x_test_selected[indicies], ben_x, torch.Tensor(manipulation)[idx], None, 1)\n",
    "print((success_flag==0).sum()/len(success_flag))\n",
    "success_flag,x_mod = mimicry(nn_selected, x_test_selected[indicies], ben_x, torch.Tensor(manipulation)[idx], None, 10)\n",
    "print((success_flag==0).sum()/len(success_flag))\n",
    "success_flag,x_mod = mimicry(nn_selected, x_test_selected[indicies], ben_x, torch.Tensor(manipulation)[idx], None, 30)\n",
    "print((success_flag==0).sum()/len(success_flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f7b57",
   "metadata": {},
   "source": [
    "## NN-Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db4784",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_counts = np.array(x_train_.sum(axis=0))[0]\n",
    "dense_feature = feature_counts.argsort()[-682:] #Features with at least 1% density\n",
    "x_train_d = np.array(x_train[:,dense_feature].todense())\n",
    "x_test_d = np.array(x_test[:,dense_feature].todense())\n",
    "x_val_d = np.array(x_val[:,dense_feature].todense())\n",
    "joblib.dump([x_train_d,x_val_d,x_test_d,y_train,y_val,y_test],'materials/drebin_2015_682.pkl')\n",
    "#x_train_d,x_val_d,x_test_d,y_train,y_val,y_test = joblib.load('materials/drebin_2015_682.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6fe47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('models/drebin/nn_drebin_d.pkl'):\n",
    "    nn_d = model_utils.load_model('nn','drebin','models/drebin/','nn_drebin_d',x_train_d.shape[1])\n",
    "else:\n",
    "    nn_d = model_utils.train_model('nn','drebin',x_train_d,y_train,x_test_d,y_test,300,'')\n",
    "    model_utils.save_model('nn',nn_d,'models/drebin/','nn_drebin_d')\n",
    "r=nn_d.predict(x_test_d)>0.5\n",
    "print(classification_report(r,y_test,digits=5))\n",
    "f1_list_d, aut_d = evaluate_aut(nn_d,x_test_d,y_test,year='2015')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ff480a",
   "metadata": {},
   "source": [
    "#### Saving modifiable features for PAD and backdoor attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e4087",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, feature_names, name_feat, feat_name = data_utils.load_features([],'drebin','2015',False,dense_feature)\n",
    "api_idx = [i for i in range(len(feature_names)) if 'calls' in feature_names[i].split('::')[0]]\n",
    "removable_fs = [i for i in range(len(feature_names)) if 'api_calls' in feature_names[i].split('::')[0]\n",
    "               or 'activities' in feature_names[i].split('::')[0] \n",
    "                or 's_and_r' in feature_names[i].split('::')[0]\n",
    "               or 'providers' in feature_names[i].split('::')[0]]\n",
    "manipulation = np.zeros(682)\n",
    "manipulation[removable_fs] = 1\n",
    "apis = np.zeros(682)\n",
    "apis[api_idx] = 1\n",
    "joblib.dump([apis,manipulation],\"materials/feat_info_dense.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42db5836",
   "metadata": {},
   "source": [
    "#### Mimicry on NN-dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6867b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = dense_feature\n",
    "indicies = np.where((r!=0)&(y_test!=0))[0][:1000]\n",
    "r_train = nn_d.predict(x_train_d)>0.5\n",
    "ben_x = x_train_d[np.where((r_train==0)&(y_train==0))]\n",
    "\n",
    "success_flag,x_mod = mimicry(nn_d, x_test_d[indicies], ben_x, torch.Tensor(manipulation)[idx], None, 1)\n",
    "print((success_flag==0).sum()/len(success_flag))\n",
    "success_flag,x_mod = mimicry(nn_d, x_test_d[indicies], ben_x, torch.Tensor(manipulation)[idx], None, 10)\n",
    "print((success_flag==0).sum()/len(success_flag))\n",
    "success_flag,x_mod = mimicry(nn_d, x_test_d[indicies], ben_x, torch.Tensor(manipulation)[idx], None, 30)\n",
    "print((success_flag==0).sum()/len(success_flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f4982b",
   "metadata": {},
   "source": [
    "## NN-Bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff09f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_bundle_,x_test_bundle,y_train_bundle_,y_test,processor = data_utils.load_compressed_drebin(2015,4)\n",
    "x_train_bundle,x_val_bundle,y_train,y_val = train_test_split(x_train_bundle_,y_train_bundle_,test_size=0.05,random_state = 3)\n",
    "if os.path.exists('models/drebin/nn_drebin_bundle.pkl'):\n",
    "    nn_bundle = model_utils.load_model('nn','drebin','models/drebin/','nn_drebin_bundle',x_train_bundle.shape[1])\n",
    "else:\n",
    "    nn_bundle = model_utils.train_model('nn','drebin',x_train_bundle,y_train,x_val_bundle,y_val,300,'')\n",
    "    model_utils.save_model('nn',nn_bundle,'models/drebin/','nn_drebin_bundle')\n",
    "r=nn_bundle.predict(x_test_bundle)>0.5\n",
    "print(classification_report(r,y_test,digits=5))\n",
    "f1_list_bundle, aut_bundle = evaluate_aut(nn_bundle,x_test_bundle,y_test,year='2015')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f212d238",
   "metadata": {},
   "source": [
    "#### Mimicry on NN-bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e0a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_list_bundle, aut_bundle)\n",
    "r=nn_bundle.predict(x_train_bundle)>0.5\n",
    "indicies = np.where((r!=0)&(y_train!=0))[0][:1000]\n",
    "ben_x = x_train[np.where((r==0)&(y_train==0))]\n",
    "\n",
    "apis,manipulation = joblib.load(\"materials/feat_info.pkl\")\n",
    "\n",
    "success_flag,x_mod = mimicry(nn_bundle, x_train[indicies][:500], ben_x, torch.Tensor(manipulation), processor, 1)\n",
    "print((success_flag==1).sum()/len(success_flag))\n",
    "success_flag,x_mod = mimicry(nn_bundle, x_train[indicies][:500], ben_x, torch.Tensor(manipulation), processor, 10)\n",
    "print((success_flag==1).sum()/len(success_flag))\n",
    "success_flag,x_mod = mimicry(nn_bundle, x_train[indicies][:500], ben_x, torch.Tensor(manipulation), processor, 30)\n",
    "print((success_flag==1).sum()/len(success_flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d504dd9",
   "metadata": {},
   "source": [
    "### NN-Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8df6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'density0'\n",
    "if os.path.exists(f'models/drebin/nn_drebin_{method}.pkl'):\n",
    "    nn_density = model_utils.load_model('nn','drebin','models/drebin/',f'nn_drebin_{method}',x_train_bundle.shape[1])\n",
    "else:\n",
    "    nn_density = model_utils.train_model('nn','drebin',x_train_bundle,y_train,x_val_bundle,y_val,300,f'{method}')\n",
    "    model_utils.save_model('nn',nn_density,'models/drebin/',f'nn_drebin_{method}')\n",
    "r=nn_density.predict(x_test_bundle)>0.5\n",
    "print(classification_report(r,y_test,digits=5))\n",
    "f1_list_density, aut_density= evaluate_aut(nn_density,x_test_bundle,y_test,year='2015')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a0735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r=nn_density.predict(x_train_bundle)>0.5\n",
    "indicies = np.where((r!=0)&(y_train!=0))[0][:1000]\n",
    "ben_x = x_train[np.where((r==0)&(y_train==0))]\n",
    "\n",
    "apis,manipulation = joblib.load(\"materials/feat_info.pkl\")\n",
    "\n",
    "success_flag,x_mod = mimicry(nn_density, x_train[indicies][:500], ben_x, torch.Tensor(manipulation), processor, 1)\n",
    "print((success_flag==1).sum()/len(success_flag))\n",
    "success_flag,x_mod = mimicry(nn_density, x_train[indicies][:500], ben_x, torch.Tensor(manipulation), processor, 10)\n",
    "print((success_flag==1).sum()/len(success_flag))\n",
    "success_flag,x_mod = mimicry(nn_density, x_train[indicies][:500], ben_x, torch.Tensor(manipulation), processor, 30)\n",
    "print((success_flag==1).sum()/len(success_flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97414bdd",
   "metadata": {},
   "source": [
    "### Draw the lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8b1cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_list, aut = [0.7625899280575541, 0.7520661157024793, 0.7312348668280872, 0.6946107784431138, 0.7667560321715818, 0.6337209302325582, 0.6973684210526315, 0.7014613778705637, 0.7166123778501629, 0.6717850287907869, 0.5922165820642977, 0.5321100917431192, 0.4590818363273452, 0.4797601199400299, 0.4585152838427948, 0.41696113074204944, 0.49695121951219506, 0.5104408352668214, 0.5619223659889093, 0.508833922261484, 0.5663716814159292, 0.5116279069767441, 0.3225806451612903, 0.4, 0.2408111533586819, 0.295, 0.24142312579415504, 0.3030998851894374, 0.24009603841536614, 0.372960372960373, 0.4823906083244397, 0.421978021978022, 0.41683366733466937, 0.46105919003115264, 0.4206008583690987, 0.4237837837837838, 0.4362549800796813, 0.4041884816753927, 0.2968845448992059, 0.38078291814946613, 0.3491755577109602, 0.503157894736842, 0.4684491978609625, 0.48297872340425535, 0.5166130760986066, 0.19546742209631726, 0.10829103214890018, 0.047058823529411764], 0.46915094596549417\n",
    "f1_list_selected,aut_selected = [0.7626459143968871, 0.7074235807860262, 0.6975609756097562, 0.6162790697674418, 0.6441558441558441, 0.562189054726368, 0.6303501945525292, 0.5996343692870201, 0.7419354838709677, 0.696461824953445, 0.5649913344887348, 0.48648648648648646, 0.4665391969407266, 0.5270457697642164, 0.4, 0.4260869565217391, 0.5620736698499318, 0.4943310657596373, 0.5916955017301038, 0.5219399538106235, 0.4545454545454545, 0.5891472868217054, 0.26666666666666666, 0.2222222222222222, 0.3047375160051216, 0.3341708542713568, 0.2912621359223301, 0.3352872215709261, 0.2518518518518519, 0.3990208078335373, 0.501138952164009, 0.44815256257449343, 0.47513812154696133, 0.5197860962566845, 0.45999999999999996, 0.42224744608399545, 0.4131147540983606, 0.4018161180476731, 0.29820051413881743, 0.3988657844990548, 0.38469493278179945, 0.5329087048832271, 0.506050605060506, 0.5617021276595744, 0.603537981269511, 0.25815602836879437, 0.1861252115059222, 0.08], 0.47189470742362977\n",
    "f1_list_d, aut_d = [0.8074074074074075, 0.6923076923076923, 0.6962962962962963, 0.6118980169971672, 0.6582914572864321, 0.5440806045340051, 0.62109375, 0.6338028169014085, 0.6656101426307448, 0.6990990990990991, 0.6466876971608833, 0.5894039735099338, 0.5225563909774437, 0.5072046109510087, 0.5141065830721003, 0.5372168284789643, 0.5563480741797432, 0.6299559471365639, 0.6330434782608696, 0.6596858638743456, 0.6506024096385542, 0.5238095238095238, 0.47058823529411764, 0.2222222222222222, 0.3424317617866005, 0.3337515683814304, 0.31013431013431014, 0.4078341013824885, 0.35990338164251207, 0.4258373205741627, 0.48914285714285716, 0.47206385404789053, 0.4718853362734289, 0.505795574288725, 0.4924406047516199, 0.45336225596529284, 0.43373493975903615, 0.31490384615384615, 0.37446808510638296, 0.4889705882352941, 0.4649805447470817, 0.6456852791878173, 0.6215384615384615, 0.6484848484848484, 0.6613065326633166, 0.35135135135135137, 0.25, 0.18181818181818182], 0.5170325938900143\n",
    "f1_list_bundle, aut_bundle = [0.7928571428571428, 0.7160493827160493, 0.6894865525672371, 0.7024128686327078, 0.6649874055415617, 0.5357142857142857, 0.6666666666666666, 0.6420664206642066, 0.6708268330733229, 0.6546052631578947, 0.617816091954023, 0.5978090766823161, 0.6256239600665557, 0.5541838134430727, 0.6218009478672986, 0.5254413291796469, 0.539454806312769, 0.5528089887640449, 0.6068222621184919, 0.6885919835560124, 0.6666666666666666, 0.4915254237288136, 0.3333333333333333, 0.4, 0.33985330073349634, 0.3130434782608696, 0.2911392405063291, 0.4165733482642777, 0.2847301951779564, 0.4004424778761062, 0.48497854077253216, 0.4323189926547744, 0.5118679050567595, 0.48502994011976047, 0.514456630109671, 0.47731755424063116, 0.4854368932038835, 0.4133738601823708, 0.3561470215462611, 0.5095119933829612, 0.4949771689497717, 0.6199324324324325, 0.631762652705061, 0.6655290102389079, 0.6878761822871883, 0.31422018348623854, 0.2136986301369863, 0.16666666666666666], 0.5231632744573215\n",
    "f1_list_density, aut_density = [0.8489208633093526, 0.7654320987654321, 0.7215496368038741, 0.6611111111111111, 0.7619047619047619, 0.6069651741293532, 0.6856060606060606, 0.691358024691358, 0.7689873417721519, 0.8, 0.7712609970674487, 0.7538461538461538, 0.762987012987013, 0.7005208333333334, 0.7078972407231209, 0.6486486486486487, 0.6535764375876578, 0.6203904555314533, 0.6382252559726962, 0.7492447129909365, 0.7251908396946565, 0.5950413223140496, 0.41379310344827586, 0.4, 0.4336175395858709, 0.4224343675417661, 0.40441176470588236, 0.47520184544405997, 0.41092636579572445, 0.4960362400906002, 0.509009009009009, 0.4759725400457666, 0.4849162011173184, 0.5238095238095238, 0.5231431646932185, 0.488272921108742, 0.49060773480662984, 0.3104693140794224, 0.40460763138948885, 0.5596244131455399, 0.5383141762452107, 0.6965648854961832, 0.6467757459095284, 0.720532319391635, 0.7423076923076923, 0.34850455136540964, 0.19829059829059828, 0.1794871794871795], 0.5834487615043112\n",
    "\n",
    "plt.style.use('bmh')\n",
    "name = \"images/drebin_features\"\n",
    "fig = plt.figure(figsize=(6,4)) #创建绘图对象\n",
    "ax = fig.add_subplot(111)\n",
    "plt.grid(axis='x')\n",
    "ax.patch.set_alpha(0)\n",
    "series = range(1,48)\n",
    "ax.set_xlabel(\"Timeline\",fontsize=15)\n",
    "ax.set_ylabel('F1 Score',fontsize=15)\n",
    "line1 = ax.plot(range(1,len(f1_list)+1),f1_list,linestyle='-.',alpha=0.5,label=f\"SVM (AUT:{aut:.5f})\")[0]\n",
    "line2 = ax.plot(range(1,len(f1_list)+1),f1_list_selected,linestyle='-.',alpha=0.5,label=f\"NN-Selected (AUT:{aut_selected:.5f})\")[0]\n",
    "line3 = ax.plot(range(1,len(f1_list)+1),f1_list_d,linestyle='-.',alpha=0.5,label=f\"NN-Density (AUT:{aut_d:.5f})\")[0]\n",
    "line4 = ax.plot(range(1,len(f1_list)+1),f1_list_bundle,linestyle='-.',alpha=0.5,label=f\"NN-Bundle (AUT:{aut_bundle:.5f})\")[0]\n",
    "line5 = ax.plot(range(1,len(f1_list)+1),f1_list_density,linestyle='-',alpha=1,label=f\"NN-DB (AUT:{aut_density:.5f})\")[0]\n",
    "\n",
    "second_legend_handles = [line1, line5,line2,line3, line4]\n",
    "second_legend_labels = [line1.get_label(), line5.get_label(), line2.get_label(), line3.get_label(), line4.get_label()]\n",
    "ax.legend(handles=second_legend_handles, labels=second_legend_labels, loc='lower left', fontsize=11, facecolor=\"none\", frameon=False)\n",
    "\n",
    "x_ticks = np.arange(1, len(f1_list), 3)  # 设置要显示的刻度位置\n",
    "plt.xticks(x_ticks,x_ticks, fontsize=13,rotation=30)\n",
    "plt.yticks(np.arange(0,0.9,0.1),fontsize=13)\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "\n",
    "plt.savefig(name+\".pdf\", bbox_inches='tight') #保存图\n",
    "plt.savefig(name, bbox_inches='tight') #保存图\n",
    "plt.show()  #显示图"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d445572d",
   "metadata": {},
   "source": [
    "## PAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e5eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pad.core.defense import AMalwareDetectionPAD\n",
    "from pad.core.defense import AdvMalwareDetectorICNN\n",
    "from pad.core.defense import MalwareDetectionDNN\n",
    "\n",
    "name = '20240605-113157'#'basepad'\n",
    "#name = '20240623-231424'#'basepad+db+pad with restricted features \n",
    "#name = '20240623-231404'#'basepad+pad with restricted features \n",
    "args = {'dense_hidden_units':[1024,512,256],\n",
    "        'dropout':0.6,\n",
    "        'alpha_':0.2,\n",
    "        'smooth':False,\n",
    "        'proc_number':10,\n",
    "       }\n",
    "model = MalwareDetectionDNN(558,\n",
    "                            2,\n",
    "                            device='cpu',\n",
    "                            name=name,\n",
    "                            **args\n",
    "                            )\n",
    "model = AdvMalwareDetectorICNN(model,\n",
    "                            input_size=558,\n",
    "                            n_classes=2,\n",
    "                            device='cpu',\n",
    "                            name=name,\n",
    "                            **args\n",
    "                            )\n",
    "max_adv_training_model = AMalwareDetectionPAD(model, None, None)\n",
    "max_adv_training_model.load()\n",
    "print(f'Load {name} pad')\n",
    "\n",
    "#evaluate performance\n",
    "y_cent, y_prob, y_true = model.inference(utils.data_iter(256,x_test_bundle, y_test, False))\n",
    "r = y_cent.argmax(axis=1)\n",
    "f1_list_pad, aut_pad= evaluate_aut(r.numpy(),y_test,year='2015')\n",
    "print(classification_report(r,y_test,digits=5))\n",
    "\n",
    "#evaluate mimicry\n",
    "y_cent, y_prob, y_true = model.inference(utils.data_iter(256,x_train_bundle, y_train, False))\n",
    "r = y_cent.argmax(axis=1)\n",
    "indicies = np.where((r!=0)&(y_train!=0))[0][:1000] #get 1000 malwares in training set\n",
    "ben_x = x_train[np.where((r==0)&(y_train==0))] #get goodwares\n",
    "print(classification_report(r,y_train,digits=5))\n",
    "\n",
    "success_flag,x_mod = mimicry(model, x_train[indicies][:500], ben_x, torch.Tensor(manipulation), processor, 1)\n",
    "print((success_flag==1).sum()/len(success_flag))\n",
    "success_flag,x_mod = mimicry(model, x_train[indicies][:500], ben_x, torch.Tensor(manipulation), processor, 10)\n",
    "print((success_flag==1).sum()/len(success_flag))\n",
    "success_flag,x_mod = mimicry(model, x_train[indicies][:500], ben_x, torch.Tensor(manipulation), processor, 30)\n",
    "print((success_flag==1).sum()/len(success_flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0435c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate backdoors\n",
    "f_s,v_s=joblib.load('materials/trigger_VR_drebin_nn_drebin_bundle_all.pkl')\n",
    "fn=16\n",
    "idx = np.where(np.array(v_s)!=0)[0]\n",
    "if len(idx)<fn:\n",
    "    v_s = [1]*len(f_s)\n",
    "    idx = np.where(np.array(v_s)!=0)[0]\n",
    "f_s = np.array(f_s)[idx].tolist()[:fn]\n",
    "v_s = np.array(v_s)[idx].tolist()[:fn]\n",
    "\n",
    "x_t = x_test_bundle[y_test==1].copy()\n",
    "y_cent, y_prob, y_true = model.inference(utils.data_iter(256,x_t, np.array([1]*x_t.shape[0]), False))\n",
    "r = y_cent.argmax(axis=1)\n",
    "print(r[r==y_true].shape[0]/r.shape[0])#performance before backdoor\n",
    "\n",
    "#inject backdoor\n",
    "for i,j in list(zip(f_s,v_s))[:16]:\n",
    "    x_t[:,i] = j\n",
    "    \n",
    "y_cent, y_prob, y_true = model.inference(utils.data_iter(256,x_t, np.array([1]*x_t.shape[0]), False))\n",
    "r = y_cent.argmax(axis=1)\n",
    "print(r[r==y_true].shape[0]/r.shape[0])#performance after backdoor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e53bf97",
   "metadata": {},
   "source": [
    "### Evaluate PAD on NN-selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '20240703-194850'\n",
    "args = {'dense_hidden_units':[1024,512,256],\n",
    "        'dropout':0.6,\n",
    "        'alpha_':0.2,\n",
    "        'smooth':False,\n",
    "        'proc_number':10,\n",
    "       }\n",
    "model = MalwareDetectionDNN(1000,\n",
    "                            2,\n",
    "                            device='cpu',\n",
    "                            name=name,\n",
    "                            **args\n",
    "                            )\n",
    "model = AdvMalwareDetectorICNN(model,\n",
    "                            input_size=1000,\n",
    "                            n_classes=2,\n",
    "                            device='cpu',\n",
    "                            name=name,\n",
    "                            **args\n",
    "                            )\n",
    "max_adv_training_model = AMalwareDetectionPAD(model, None, None)\n",
    "max_adv_training_model.load()\n",
    "print(f'Load {name} pad')\n",
    "\n",
    "y_cent, y_prob, y_true = model.inference(utils.data_iter(256,x_test_selected, y_test, False))\n",
    "r = y_cent.argmax(axis=1)\n",
    "f1_list_pad, aut_pad= evaluate_aut_r(r.numpy(),y_test,year='2015')#AUT: 50.3%\n",
    "\n",
    "y_cent, y_prob, y_true = model.inference(utils.data_iter(256,x_train_selected, y_train, False))\n",
    "r = y_cent.argmax(axis=1)\n",
    "indicies = np.where((r!=0)&(y_true!=0))[0][:1000]\n",
    "#r_test = nn_bundle.predict(x_test_bundle)>0.5\n",
    "ben_x = x_train_selected[np.where((r==0)&(y_true==0))]\n",
    "\n",
    "success_flag,x_mod = mimicry(model, x_train_selected[indicies][:500], ben_x, torch.Tensor(manipulation[idx]), None, 1, False)\n",
    "print((success_flag==1).sum()/len(success_flag))\n",
    "success_flag,x_mod = mimicry(model, x_train_selected[indicies][:500], ben_x, torch.Tensor(manipulation[idx]), None, 10, False)\n",
    "print((success_flag==1).sum()/len(success_flag))\n",
    "success_flag,x_mod = mimicry(model, x_train_selected[indicies][:500], ben_x, torch.Tensor(manipulation[idx]), None, 30, False)\n",
    "print((success_flag==1).sum()/len(success_flag))\n",
    "\n",
    "f_s,v_s=joblib.load('materials/trigger_VR_drebin_nn_drebin_selected_all.pkl')\n",
    "x_t = x_test_selected[y_test==1].copy()\n",
    "y_cent, y_prob, y_true = model.inference(utils.data_iter(256,x_t, np.array([1]*x_t.shape[0]), False))\n",
    "r = y_cent.argmax(axis=1)\n",
    "\n",
    "fn=16\n",
    "idx = np.where(np.array(v_s)!=0)[0]\n",
    "if len(idx)<fn:\n",
    "    v_s = [1]*len(f_s)\n",
    "    idx = np.where(np.array(v_s)!=0)[0]\n",
    "f_s = np.array(f_s)[idx].tolist()[:fn]\n",
    "v_s = np.array(v_s)[idx].tolist()[:fn]\n",
    "\n",
    "for i,j in list(zip(f_s,v_s))[:16]:\n",
    "    x_t[:,i] = j\n",
    "\n",
    "y_cent, y_prob, y_true = model.inference(utils.data_iter(256,x_t, np.array([1]*x_t.shape[0]), False))\n",
    "r = y_cent.argmax(axis=1)\n",
    "r[r==y_true].shape[0]/r.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fd4311",
   "metadata": {},
   "source": [
    "## Evaluation on APIGraph "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eb456e",
   "metadata": {},
   "source": [
    "### Process datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "cb7c91f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from scipy.sparse import vstack\n",
    "vec = DictVectorizer()\n",
    "X = vec.fit_transform(X)\n",
    "y = np.asarray(y)\n",
    "# Partition dataset via TESSERACT\n",
    "splits = temporal.time_aware_train_test_split(X, y, t, train_size=12, test_size=1, granularity='month')\n",
    "\n",
    "x_train_, x_test, y_train_, y_test, t_train, t_test = splits\n",
    "x_test = vstack(x_test)\n",
    "y_test = np.hstack(y_test)\n",
    "\n",
    "feature_names = vec.get_feature_names_out()\n",
    "\n",
    "#Download the APIGrap first\n",
    "with open('APIGraph/src/res/method_cluster_mapping_%d.pkl' % (2000), 'rb') as f:\n",
    "    method_cluster_mapping = pickle.load(f) \n",
    "#method_cluster_mapping = {key.lower(): value for key, value in method_cluster_mapping.items()}\n",
    "\n",
    "api_idx = [i for i in range(len(feature_names)) if 'calls' in feature_names[i].split('::')[0]]\n",
    "api_features = [feature_names[i] for i in api_idx]\n",
    "x_apis = x_train_[:,api_idx].todense()\n",
    "x_apis_test = x_test[:,api_idx].todense()\n",
    "\n",
    "#Some APIs are not in current list, we do a simple procession for them\n",
    "processed_apis = []\n",
    "count = 0\n",
    "count1 = 0\n",
    "for idx,api in enumerate(api_features):\n",
    "    api_type = api.split('::')[0]\n",
    "    api = api.split('::')[1]\n",
    "    api = api.replace('/','.').replace(';->','.')\n",
    "    if 'Cipher' in api:#all cipher are compressed into 1 feature\n",
    "        processed_apis.append(1761)\n",
    "    elif api in method_cluster_mapping:\n",
    "        processed_apis.append(method_cluster_mapping[api])\n",
    "        count1 += 1\n",
    "    else:\n",
    "        print(idx,api)\n",
    "        while True:\n",
    "            candidates = [(i,method_cluster_mapping[i],Levenshtein.ratio(api, i)) for i in method_cluster_mapping if api in i]\n",
    "            if len(candidates) == 0 and '.' in api:\n",
    "                api = '.'.join(api.split('.')[:-1])\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        candidates = sorted(candidates,key=lambda x:x[2],reverse=True)[:5]\n",
    "        #candidates = [(i,method_cluster_mapping[i],Levenshtein.distance(api,idx)) for i in method_cluster_mapping if api in i]\n",
    "        print(candidates)\n",
    "        if candidates:\n",
    "            processed_apis.append(candidates[0][1])\n",
    "        else:\n",
    "            processed_apis.append(api)\n",
    "        count += 1\n",
    "\n",
    "papiset = set(processed_apis)\n",
    "print(len(papiset))\n",
    "new_f = []\n",
    "new_f_test = []\n",
    "for i in papiset:\n",
    "    indicies = [idx for idx,api in enumerate(processed_apis) if api == i]\n",
    "    test_indicies = [idx for idx,api in enumerate(processed_apis) if api == i]\n",
    "    new_f.append(np.array(x_apis[:,indicies].sum(axis=1)))\n",
    "    new_f_test.append(np.array(x_apis_test[:,indicies].sum(axis=1)))\n",
    "new_x = np.hstack(new_f)\n",
    "new_x_test = np.hstack(new_f_test)\n",
    "new_x[new_x>1] = 1\n",
    "new_x_test[new_x_test>1] = 1\n",
    "\n",
    "x_train_api,x_val_api,y_train,y_val = train_test_split(new_x,y_train_,test_size=0.05,random_state = 3)\n",
    "x_test_api = new_x_test\n",
    "\n",
    "#Only use API features to train model\n",
    "if not os.path.exists('models/drebin/nn_drebin_api.pkl'):\n",
    "    nn_api = model_utils.load_model('nn','drebin','models/drebin/','nn_drebin_api',x_train_api.shape[1])\n",
    "else:\n",
    "    nn_api = model_utils.train_model('nn','drebin',x_train_api,y_train,x_val_api,y_val,200,'density0')\n",
    "    model_utils.save_model('nn',nn_api,'models/drebin/','nn_drebin_api')\n",
    "r=nn_api.predict(x_test_api)>0.5\n",
    "print(classification_report(r,y_test,digits=5))\n",
    "f1_list_api, aut_api = evaluate_aut(nn_api,x_test_api,y_test,year='2015')\n",
    "#0.38562"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d423d9fe",
   "metadata": {},
   "source": [
    "### APIGraph-Dense features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "7b42ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicies = np.where(x_train_.sum(axis=0)>577)[1]\n",
    "indicies = np.array(x_train_.sum(axis=0))[0].argsort()[-10000:]\n",
    "indicies = [i for i in indicies if i not in api_idx]\n",
    "\n",
    "x_train_api = np.hstack([new_x,x_train_[:,indicies].todense()])\n",
    "x_test_api = np.hstack([new_x_test,x_test[:,indicies].todense()])\n",
    "\n",
    "\n",
    "\n",
    "print(x_train_api.shape)#735\n",
    "x_train_api,x_val_api,y_train,y_val = train_test_split(x_train_api,y_train_,test_size=0.05,random_state = 3)\n",
    "\n",
    "if not os.path.exists('models/drebin/nn_drebin_api.pkl'):\n",
    "    nn_api = model_utils.load_model('nn','drebin','models/drebin/','nn_drebin_api',x_train_api.shape[1])\n",
    "else:\n",
    "    nn_api = model_utils.train_model('nn','drebin',x_train_api,y_train,x_val_api,y_val,200,'')\n",
    "    model_utils.save_model('nn',nn_api,'models/drebin/','nn_drebin_api')\n",
    "r=nn_api.predict(x_test_api)>0.5\n",
    "print(classification_report(r,y_test,digits=5))\n",
    "f1_list_api, aut_api = evaluate_aut(nn_api,x_test_api,y_test,year='2015')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311b159d",
   "metadata": {},
   "source": [
    "## APIGraph-Bundle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc9cd9d",
   "metadata": {},
   "source": [
    "#### Save the dataset for bundling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffea675",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicies = np.where(x_train_.sum(axis=0)>=50)[1]\n",
    "indicies = [i for i in indicies if i not in api_idx]\n",
    "x_train_api = np.hstack([new_x,x_train_[:,indicies].todense()])\n",
    "x_test_api = np.hstack([new_x_test,x_test[:,indicies].todense()])\n",
    "print(len(indicies))\n",
    "x_train_api, x_test_api = np.array(x_train_api), np.array(x_test_api)\n",
    "joblib.dump([x_train_api,y_train,x_test_api,y_test],\"materials/apigraphfeatures.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "b52818fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the processed dataset\n",
    "x_train_api,x_test_api,y_train,y_test = joblib.load(f\"materials/compressed_drebin_apigraph_4.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "35e8876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_api,x_val_api,y_train,y_val = train_test_split(x_train_api,y_train_,test_size=0.05,random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f295a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('models/drebin/nn_drebin_api_bundle.pkl'):\n",
    "    nn_api = model_utils.load_model('nn','drebin','models/drebin/','nn_drebin_api_bundle',x_train_api.shape[1])\n",
    "else:\n",
    "    nn_api = model_utils.train_model('nn','drebin',x_train_api,y_train,x_val_api,y_val,200,'')\n",
    "    model_utils.save_model('nn',nn_api,'models/drebin/','nn_drebin_api_bundle')\n",
    "r=nn_api.predict(x_test_api)>0.5\n",
    "print(classification_report(r,y_test,digits=5))\n",
    "f1_list_api, aut_api = evaluate_aut(nn_api,x_test_api,y_test,year='2015')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c0d6d0",
   "metadata": {},
   "source": [
    "## APIGraph-Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "1f483c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "selector = SelectFromModel(basesvm, prefit=True, max_features=1000)\n",
    "idx = np.where(selector.get_support()==True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "3eba4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_api = np.hstack([new_x,x_train_[:,idx].todense()])\n",
    "x_test_api = np.hstack([new_x_test,x_test[:,idx].todense()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "ec0a5e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_api,x_val_api,y_train,y_val = train_test_split(x_train_api,y_train_,test_size=0.05,random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fdb9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('models/drebin/nn_drebin_api.pkl'):\n",
    "    nn_api = model_utils.load_model('nn','drebin','models/drebin/','nn_drebin_api_select',x_train_api.shape[1])\n",
    "else:\n",
    "    nn_api = model_utils.train_model('nn','drebin',x_train_api,y_train,x_val_api,y_val,200,'')\n",
    "    model_utils.save_model('nn',nn_api,'models/drebin/','nn_drebin_api_select')\n",
    "r=nn_api.predict(x_test_api)>0.5\n",
    "print(classification_report(r,y_test,digits=5))\n",
    "f1_list_api, aut_api = evaluate_aut(nn_api,x_test_api,y_test,year='2015')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
